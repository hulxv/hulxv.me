---
title: "Mill-IO: Event-loop library for Rust!"
date: 2025-08-29
description: mill-io is a lightweight event loop library for Rust that provides efficient non-blocking I/O management without relying on heavyweight async runtimes. It's a reactor-based event loop implementation built on top of mio-rs. In this article, we'll discuss it, the problem it solves, why it exists, and more!
tags: ["Rust", "Event Loop", "I/O", "Concurrency", "System Programming"]
image:
  {
    src: /imgs/posts/mill-io-event-loop-library/hero.webp,
    alt: "Birds with the crab and a mill in the background.",
  }
---

# Introduction

Have you ever wondered how large-scale systems work internally? It's a really important question. In engineering, there are a lot of techniques that are used to solve various problems. All of those have pros and cons; there is no **_perfect solution_**. Your job is to find the best solution for your case, one that has the pros that solve your problem and the fewest cons that you can bear. One of the critical problems is building a huge multi-user system. Throughout this article, we will discuss our problem and how we can solve it with our library, [mill-io](https://github.com/citadel-tech/Event-loop), an event-loop library I worked on during my Summer of Bitcoin internship to be used inside [Coinswap](https://github.com/citadel-tech/coinswap).

# Our Problem

Imagine we have a simple system, a web server. It's really simple; single-threaded and handles requests by reading/writing to the socket one by one. There's nothing special about it. If you have 1–10 users, maybe you wouldn't face any problems and you wouldn't need to implement any complex solution, but imagine those users become 1 million. Your website becomes a hell for your users due to single-threaded processing.

There's a simple solution for this problem: the `thread-per-connection` approach. Basically, we process each connection inside an individual thread concurrently to enhance performance and decrease the latency of each request. But still, we have performance issues. The OS manages thread execution with a scheduler. A thread has different states, and when it's blocked, the OS stores its context—such as CPU registers (general-purpose registers, stack pointer, and program counter), its stack, and scheduling information—inside the memory. Before it runs again, the CPU loads the thread's state from memory and stores the previous state in memory, as before. This operation is called **Context Switching**. Unfortunately, this operation takes time and have a high memory consumption per thread, and the more threads you have, the worse the performance. We have another struggle now!

# How to Solve This Problem?

To solve any problem, you'll have an initial solution. This solution solves your problem but not in the best way. We find the best solution by optimizing this basic solution. So let's optimize our solution!

The main problem we face is I/O-blocked operations, like reading/writing files. These operations take a lot of time due to the basic CPU operations, and the thread is blocked when it does these operations until it finishes.

You don't need to worry about that. The OS provides some syscalls that you can use to find the best solution. What we need is to know when files (or sockets in our web-server example) are ready for reading or writing. There are some system calls that notify us when a specific event occurs (like data being available to read) so we can run a piece of code. After that, we need to limit the number of threads to reduce the context-switching operations that appear in the `thread-per-connection` approach. Now, we can say we have a good solution!

# So, What's the Event Loop?

Now that we understand the problem and know we need a better solution, let's explore what an event loop actually is and how it solves our scalability issues.

The event loop is a programming construct that's designed specifically to handle our I/O bottleneck problem. Instead of blocking threads while waiting for I/O operations, it uses a different approach: **non-blocking I/O with event notifications**.

Think of it like this: instead of having a waiter (thread) stand by each table (connection) waiting for customers to finish reading the menu, you have one smart coordinator who watches all tables at once and only sends a waiter when a customer is actually ready to order.

**Here's how the event loop solves our problems:**

1. **Single-threaded efficiency**: One thread can monitor thousands of connections
2. **No blocking**: Instead of waiting for I/O, we get notified when it's ready
3. **Minimal context switching**: Fewer threads mean less overhead
4. **Better resource utilization**: CPU time is used for actual work, not waiting

**Step-by-Step Breakdown:**
1. Register interest in I/O events (like incoming data or connections)
2. Wait for the OS to tell us when something is ready
3. Handle the ready events quickly
4. Return to waiting for more events

This is exactly what Node.js uses to handle thousands of concurrent connections with just one thread. Libraries like [libevent](https://libevent.org/) and [libuv](https://libuv.org/) implement this pattern and power many high-performance systems.


# Internals of Polling

You might wonder: "How does the event loop actually know when I/O is ready?" The answer lies in **polling** - a key technique that makes all of this possible.

Polling is the bridge between our application and the operating system's I/O capabilities. Remember our problem with blocking I/O? Polling solves this by letting us ask the OS: "Tell me when any of these sockets are ready, but don't make me wait if none are ready right now."

## The Evolution of Polling

This wasn't always available. Early systems had limited options:

**The Old Way (select)**: The original UNIX `select` syscall was revolutionary but had limitations. You could only monitor a limited number of file descriptors, and it wasn't very efficient for large numbers of connections.

**The Better Way (poll, epoll, kqueue)**: As systems evolved, new APIs were created:
- `epoll`, `poll` (Linux)
- `kqueue` (BSD/macOS)
- `IOCP` (Windows)

## How Modern Polling Works

With modern polling APIs, your program can efficiently manage thousands of connections:

1. **Register Interest**: Tell the OS which files/sockets you care about
2. **Specify Events**: Say whether you want to know about reads, writes, or both
3. **Wait for Notifications**: The OS wakes you up only when something is ready
4. **Handle Events**: Process the ready operations without blocking

Let's see how this looks in practice with `epoll` on Linux:

```c
#include <sys/epoll.h>

// Create an epoll instance
int epfd = epoll_create1(0);

// Register a socket for read events
struct epoll_event event;
event.events = EPOLLIN;  // We want to know about reads
event.data.fd = socket_fd;
epoll_ctl(epfd, EPOLL_CTL_ADD, socket_fd, &event);

// Wait for events (this is the magic!)
struct epoll_event events[MAX_EVENTS];
int num_events = epoll_wait(epfd, events, MAX_EVENTS, -1);

// Handle ready events
for (int i = 0; i < num_events; i++) {
    if (events[i].events & EPOLLIN) {
        // Socket is ready for reading - no blocking!
        handle_read(events[i].data.fd);
    }
}
```

This is exactly what makes our event loop solution so powerful - we're not constantly checking (busy-waiting) or blocking. We get precise notifications only when work is available.

In Rust with `mio` (which mill-io uses), this becomes even cleaner:

```rust
use mio::{Events, Interest, Poll, Token};

let mut poll = Poll::new()?;
let mut events = Events::with_capacity(1024);

// Register socket for read events
poll.registry().register(&mut socket, Token(0), Interest::READABLE)?;

// Wait for events - non-blocking and efficient
poll.poll(&mut events, None)?;

for event in events.iter() {
    match event.token() {
        Token(0) => {
            // Our socket is ready to read!
            handle_socket_read();
        }
        _ => unreachable!(),
    }
}
```

Modern event loops like mill-io use libraries like `mio` to abstract these platform differences, giving you the best available polling mechanism on any system.

# Other Solutions

Event loops aren't the only way to handle concurrency, especially for I/O-bound tasks. There are many approaches to solve our problem:

- **Multithreading**: Using multiple threads, where each thread handles a separate task. This is the traditional approach to concurrency, which we discussed before. It has some problems but is still a good solution.
- **Multiprocessing**: Using multiple processes, each with its own memory space, to handle different tasks. This is a robust solution that provides better isolation.
- **Asynchronous I/O with Callbacks**: This is the model the event loop uses, but without the central loop. Instead, each I/O request is made with a callback function that is invoked when the operation completes.
- **Asynchronous I/O with Coroutines/Generators**: This is a more modern approach that uses language features to make asynchronous code look like synchronous code, making it easier to read and write.


# How mill-io Works and Why It Exists?

This project was built during my [Summer of Bitcoin](https://www.summerofbitcoin.org/) internship (I will write an article about it soon) for the [Citadel-tech](https://github.com/citadel-tech) organization to be used inside the [Coinswap project](https://github.com/citadel-tech/coinswap) to enhance its performance through better I/O operation management without relying on heavyweight async runtimes such as [tokio-rs](https://tokio.rs/) and [async-std](https://github.com/async-rs/async-std). The implementation leverages **mio**'s polling capabilities to create a [reactor-based architecture](https://en.wikipedia.org/wiki/Reactor_pattern) with a configurable thread pool, ensuring **Coinswap**'s core logic remains runtime-agnostic while achieving optimal performance and resource utilization.

# Mill-IO Architecture 

Let's break down how mill-io works behind the scenes. The event loop is made up of a few simple building blocks that work together to handle lots of connections efficiently. Here's a quick overview:

- **Application Layer**: This is your code! You write event handlers and use the EventLoop API to register what you want to listen for.
- **Mill-IO Core**: The heart of the library. It includes the Reactor (the boss), PollHandle (the listener), ThreadPool (the workers), and a Handler Registry (keeps track of who does what).
- **System Layer**: This is where mill-io talks to the operating system using mio, which uses fast polling APIs like epoll, kqueue, or IOCP.
- **Memory Management**: Handles buffers and objects efficiently so things run smoothly.

Here's a visual to help you see how everything connects:

```mermaid
graph TB
    subgraph "Application Layer"
        A1[User Code] --> A2[EventHandler Implementation]
        A2 --> A3[EventLoop API]
    end

    subgraph "Mill-IO Core"
        A3 --> B1[Reactor]
        B1 --> B2[PollHandle]
        B1 --> B3[ThreadPool]
        B2 --> B4[Handler Registry]
    end

    subgraph "System Layer"
        B2 --> C1[mio::Poll]
        C1 --> C2[OS Polling]
        C2 --> C3[epoll/kqueue/IOCP]
    end

    subgraph "Memory Management"
        B3 --> D1[ObjectPool]
        D1 --> D2[Buffer Recycling]
    end
```

## 1. PollHandle – Listening for Events

PollHandle is like a receptionist who keeps an eye on all your sockets and files. It uses the mio library to ask the OS, "Let me know when something interesting happens!" When an event is ready, PollHandle knows which handler to call.

**Example:**
```rust
let poller = PollHandle::new().unwrap();
let mut src = TestSource::new();

struct NoopHandler;
impl EventHandler for NoopHandler {
    fn handle_event(&self, _event: &mio::event::Event) {
        println!("Handling the Noop event...");
    }
}

poller
    .register(&mut src, mio::Token(1), mio::Interest::READABLE, NoopHandler)
    .expect("Failed to register src");
```

## 2. ThreadPool – The Workers

Once an event is ready, we need someone to do the work. That's where the thread pool comes in. Instead of creating a new thread for every task (which is slow and uses lots of memory), we keep a small team of worker threads ready to go. When a job comes in, one of them grabs it and gets to work.

**Example:**
```rust
let pool = ThreadPool::new(4);
let counter = Arc::new(AtomicUsize::new(0));

for _ in 0..10 {
    let counter_clone = counter.clone();
    pool.exec(move || {
        counter_clone.fetch_add(1, Ordering::SeqCst);
    })
    .unwrap();
}
```

## 3. Reactor – The Boss

The Reactor is the coordinator. It listens for events using PollHandle and then tells the thread pool to run the right handler. It's like a dispatcher making sure every event gets handled quickly and efficiently.

**How it works:**
- Polls for events
- Dispatches them to the thread pool
- Makes sure everything keeps running smoothly

```mermaid
flowchart LR
    A[Reactor] --> B[Poll Events]
    B --> C[Dispatch to ThreadPool]
    C --> D[Execute Handlers]
    D --> B

    A --> E[Shutdown Signal]
    E --> F[Graceful Stop]
```

## 4. EventLoop – The Simple Interface

You don't need to worry about all the technical details. The EventLoop gives you an easy way to register events, start the loop, and shut things down when you're done. It manages the Reactor and keeps everything running in the background.

**How it feels to use:**
- Register your events and handlers
- Start the event loop
- Your handlers get called automatically when events happen
- Stop the loop when you're finished


If you want to understand the fully flow of the event-loop, you can talk a look on this diagram:

```mermaid
sequenceDiagram
    participant App as Application
    participant EL as EventLoop
    participant R as Reactor
    participant PH as PollHandle
    participant TP as ThreadPool
    participant H as Handler

    App->>EL: register(source, token, handler)
    EL->>R: poll_handle.register()
    R->>PH: register with mio + store handler

    Note over R: Main event loop
    loop Event Loop
        R->>PH: poll(events, timeout)
        PH->>R: return event count

        loop For each event
            R->>TP: dispatch_event(event)
            TP->>H: handle_event(event)
            H-->>TP: processing complete
        end
    end

    App->>EL: stop()
    EL->>R: shutdown_handle.shutdown()
    R->>PH: wake()
    Note over R: Exit event loop
```




# Acknowledgments

This project was developed as part of the **Summer of Bitcoin 2025** program. Special thanks to:

- [Citadel-tech](https://github.com/citadel-tech) and the [Coinswap](https://github.com/citadel-tech/coinswap) project for providing the use case and requirements.
- **Summer of Bitcoin** organizers and mentors for their guidance.
- The **mio** project for providing the foundational polling abstractions.

# Helpful Resources

- [Mill-io](https://github.com/citadel-tech/Event-loop)
- [Event Loop Programming: A Different Way of Thinking](https://linuxjedi.co.uk/event-loop-programming-a-different-way-of-thinking)
- [Event Loop mechanism and how it works](https://raypvn.medium.com/event-loop-mechanism-and-how-it-works-b038279778c4)
- [JavaScript execution model - MDN - Mozilla](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Execution_model)
